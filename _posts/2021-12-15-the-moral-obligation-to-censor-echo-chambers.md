---
layout: post
categories:
- writings
title: The Moral Obligation to Censor Echo Chambers
author: sami
image: "/assets/images/echo-chamber.jpeg"

---
In our ‘post-truth’ world, it is now more critical than ever to understand the proper balance between unabated speech and the censorship of harmful ideologies. With the advent of modern technology, communication barriers between people have nearly vanished. This has allowed for some very pleasant and unexpected communities to flourish — where else but the internet can you find hundreds of strangers ready to discuss intricate knitting techniques at a moment's notice? But along with these harmless discussion boards are a plethora of bad faith actors who leverage the anonymity and ease-of-use of the internet to spread misinformation, creating echo chambers that have tremendous impact on innocent people in the real world. Combatting those individuals who wish harm is no trivial task, and oftentimes services will resort to eradicating the entire online sub-community, making it harder for these toxic spaces to grow. These bans, despite seeming to help mitigate the growth of harmful communities, are often met with outcry, with claims that it is wrong for these private entities to censor free speech, and that places like Facebook must be impartial and not interfere with such groups. However, I believe that it is not only within Facebook’s moral right to disband these groups, it is also their moral obligation. In this essay, I will argue that entities that facilitate online discussion (i.e Facebook, Twitter, Reddit, etc.) have a moral obligation to censor echo chambers that have concrete harmful effects on society, especially if these entities employ some form of recommendation algorithm. I will draw on Mill’s four arguments against censorship, and explain why I feel they are inadequate in the face of bad faith actors and the echo chambers of today.

Mill’s appeal to the fallibility of humanity constructs a strong and compelling case for his first claim while simultaneously revealing the shortcomings of these tenets as they relate to today’s online world. Here, Mill argues that what is censored has a chance to be true, and to assert otherwise would be to falsely assume that we are infallible. This argument makes more sense in discussion of things that may be unorthodox or draw ire from reactionaries, but it hinges on some sort of ‘good faith’ to not be taken advantage of. Take for example hate speech: are we to not censor this speech purely because the underlying facts may be true? Many times hate speech is based on some true observation, but we reject it because we know that these facts are misrepresented to further a racist and discriminatory agenda. Allowing the fear of censoring some parts of the truth to prevent us from limiting hate speech with wide-spread consequences seems to be a recipe for disaster. In an ideal society composed only of good faith actors, I could be easily compelled to accept Mill’s first argument as the basis for an anti-censorship platform; however, the unideal world we reside in requires us to understand the impacts of sharing information and contextualize that within our discussions. An objective approach, as I believe Mill’s to be, fails to consider those who wish to manipulate the system for their own gain.

Allowing the patently false to be disseminated in the pursuit of the truth is a noble but ultimately ineffective stance that leads to more avenues for deliberately falsified information to gain traction. Mill argues in his second point that we should refrain from censoring speech as some part of it may be true. The issue still remains that Mill does not consider the societal harms of sharing misinformation: in fact, Mill fails to consider sharing true facts that are shown in misleading ways. Take for example the infamous “FBI Crime Statistics by race” chart that is used by people to justify their beliefs that African Americans are inherently violent. (The dog-whistle chart seems to have been scrubbed from the internet, but the data it is based off of is cited below). The chart in and of itself is true, but it purposely obfuscates information to justify the disproportionate policing and incarceration of the African-American population in the US. Again, Mill’s pursuit of objectivity fails to consider the societal impacts of such statements. Just as I believe we should not allow speech solely on the fact that it might be true, we similarly should not allow speech solely on the fact that some part of it might be true.

As we come to Mill’s final two arguments, I find myself agreeing with the thought process while reaching a slightly different conclusion. On Mill’s claim that a truth sustained through censorship will both not be understood and will be more prone to error,, I am in total agreement. However, I believe there is a slight disconnect, in which Mill believes we must allow falsehoods to persist so that people may better understand the truth, while I believe we should present these falsehoods in context of how we understand the truth. For example: we should not leave Flat Earth communities to fester and grow and spread misinformation, but we should instead incorporate the ideas that these communities are based on and show them to not be true within our current understanding. We do not ban the discussion of a flat earth in its entirety, because the discussion is in and of itself important to the understanding that the earth is _not_ flat. However, we need to make sure that this is not a viewpoint that is allowed to exist by itself in a vacuum, and must exist within the context of other information regarding the shape of the earth. Allowing the freedom to be wrong and to discuss incorrect ideas is a critical part of the ideation process of important concepts. To preserve this, however, is not to permit any and all forms of speech, but to contextualize this incorrectness with how we understand the truth, leaving the door open for the discussion of ‘wrong’ ideas while still dissuading the formation of echo chambers centered around these ideas.

Having discussed the moral duty of online discussion boards to control harmful groups within the framework of Mill’s four arguments against censorship, I will now bolster my argument outside of this framework. In addition to these entities having a general moral obligation to fight the dissemination of harmful information, this duty is multiplied tenfold when they themselves drive people into forming these communities. It is no secret that places like Facebook and Twitter employ recommendation algorithms, and furthermore it is widely accepted that these recommendations help people with extreme views find each other and form groups,. Even ignoring the argument that there is an inherent duty to break up echo chambers that parrot harmful information, it is not far-fetched to expect these companies to police the spaces that they themselves created. The hesitation to control and delete these groups is more understandable when we paint the picture of a corporate entity cracking down on an organically-formed group. But, when we better understand that these groups that harbor anything from anti-science rhetoric to downright Nazi sentiment are sizable (and growing) mainly due to the corporate entities themselves bringing these people together, it becomes more understandable, and in my opinon more of a moral duty, to break them up before they get even more out of hand. Even if one were not persuaded by my arguments drawing on Mill’s writing, I think the responsibility that these entities bear through their recommendation algorithms alone should make it their moral duty to break up these echo chambers.

The notion that the only true way to protect free speech is to allow unmolested discussion is one of days past and does not hold in the context of this post-truth world. Mill’s arguments seem to play into the classic tolerance paradox: if we want to be maximally tolerant, then we must also tolerate intolerance — but then how are we tolerant? Mill’s arguments sacrifice nuance for scope, attempting to encapsulate all speech regardless of context. In a vacuum, these points are quite powerful and could serve as the basis for free speech discussion for years to come. Outside of this vacuum, it becomes crucial to understand the effects of speech on the real world, regardless of the amount of truth encapsulated in it. The entities that control these online discussion boards have a moral obligation to stop harmful speech on their platform from festering and affecting real people, and this obligation only becomes more dire when we consider many of these online communities are a direct result of algorithms employed to maximize profit margins. “The road to hell is paved with good intentions”: we should be careful that in our pursuit of freedom we do not allow those who wish to spread harm the same platform as the rest of us.

\[Writer's Note: This was my final paper for 24.131, Ethics in Technology\]

***

References:

Cover Photo is from [this](https://royalsocietypublishing.org/doi/10.1098/rsos.181122) paper

Chowdhury & Belli. “Examining Algorithmic Amplification of Political Content on Twitter.” Accessed December 8, 2021. [https://blog.twitter.com/en_us/topics/company/2021/rml-politicalcontent.](https://blog.twitter.com/en_us/topics/company/2021/rml-politicalcontent. "https://blog.twitter.com/en_us/topics/company/2021/rml-politicalcontent.")

TechCrunch. “Facebook Partially Documents Its Content Recommendation System.” Accessed December 8, 2021. [https://social.techcrunch.com/2020/08/31/facebook-partially-documents-its-content-recommendation-system/.](https://social.techcrunch.com/2020/08/31/facebook-partially-documents-its-content-recommendation-system/ "https://social.techcrunch.com/2020/08/31/facebook-partially-documents-its-content-recommendation-system/")

Mill, John Stuart. _On Liberty_, 1859.

Mill, Kevin. “24.131 - Class 22 - Freedom of Speech,” 2021.

Nguyen, C. Thi. “ECHO CHAMBERS AND EPISTEMIC BUBBLES.” _Episteme_ 17, no. 2 (June 2020): 141–61. [https://doi.org/10.1017/epi.2018.32.](https://doi.org/10.1017/epi.2018.32. "https://doi.org/10.1017/epi.2018.32.")

Popper, K.R. _The Open Society and Its Enemies_, 1945.

The Federal Bureau of Investigation. “FBI Crime Statistics, Table 43,” 2019. [https://ucr.fbi.gov/crime-in-the-u.s/2018/crime-in-the-u.s.-2018/tables/table-43.](https://ucr.fbi.gov/crime-in-the-u.s/2018/crime-in-the-u.s.-2018/tables/table-43. "https://ucr.fbi.gov/crime-in-the-u.s/2018/crime-in-the-u.s.-2018/tables/table-43")

Whittaker, J., S. Looney, A. Reed, and F. Votta. “Recommender Systems and the Amplification of Extremist Content.” _Internet Policy Review_ 10, no. 2 (2021). [https://doi.org/10.14763/2021.2.1565.](https://doi.org/10.14763/2021.2.1565. "https://doi.org/10.14763/2021.2.1565.")